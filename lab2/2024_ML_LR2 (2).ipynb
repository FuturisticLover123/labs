{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numdifftools as nd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846de5f5",
   "metadata": {},
   "source": [
    "# <font color = 'red'> ЛР 2. Дифференцирование функций многих переменных. Линейная регрессия по произвольному базису. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be203bc0",
   "metadata": {},
   "source": [
    "Сложность: <font color = 'orange'> Нормально  </font>.\n",
    "\n",
    "Дата составления: 10.09.2024\n",
    "\n",
    "Срок выполнения: 2 недели.\n",
    "\n",
    "Автор: ст. преподаватель Кушнеров А.В."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f72d2",
   "metadata": {},
   "source": [
    "## <font color = 'green'> 1. Дифференцирование функции векторного аргумента. База.  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17525728",
   "metadata": {},
   "source": [
    "Вспоминаем понятие дифференцирования.\n",
    "\n",
    "Пусть, для начала, $f$ - функция одного аргумента, возвращающая один аргумент. Иными словами, $f:\\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "Говорят, что $f$ **дифференцируема** в точке $x_{0}$, если существует конечный предел: $$\\lim\\limits_{h \\to 0} \\frac{f(x_{0}+h) - f(x_{0})}{h} = f'(x_{0})$$.\n",
    "\n",
    "Его называют производной функции в точке $x_{0}$.  Это жу информацию можно записать в дифференциальной форме:  $$f(x_{0}+h) - f(x_{0}) = f'(x_{0}) h + o(h) = [Df_{x_{0}}](h) + o(h)$$.Величину $f'(x_{0}) h = [Df_{x_{0}}](h)$ называют **дифференциалом функции**.\n",
    "\n",
    "\n",
    "Теперь предположим, что функция $f$ работает несколько иначе. Теперь она принимает на вход векторный аргумент, а возвращает всё ещё скалярный $f:\\mathbb{R^{m}} \\rightarrow \\mathbb{R}$. \n",
    "\n",
    "Теперь понятие дифференциала вводят несколько иначе. Мы всё также можем записать: $$f(\\overline{x_{0}}+\\overline{h}) - f(\\overline{x_{0}}) = \\sum\\limits_{j=1}^m \\frac{\\partial f}{\\partial x_{m}}\\bigg\\rvert_{x=\\overline{x_{0}}}h_{i} = [Df_{x_{0}}](\\overline{h}) + o(||\\overline{h}||)$$.\n",
    "\n",
    "Или же в более сжатой форме: $$f(\\overline{x_{0}}+\\overline{h}) - f(\\overline{x_{0}}) = (\\nabla_{x_{0}} f)\\cdot\\overline{h} + o(||\\overline{h}||) $$\n",
    "\n",
    "Под производной в этом случае понимают **вектор** (чаще его пищут в виде вектор-столбца) градиента $\\nabla_{x_{0}} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{1}}\\\\ \\frac{\\partial f}{\\partial x_{2}}\\\\...\\\\\\frac{\\partial f}{\\partial x_{m}} \\end{bmatrix}\\bigg\\rvert_{x=\\overline{x_{0}}}$.\n",
    "\n",
    "В МО мы вынуждены работать с задачами оптимизации как раз таких функций, что немедленно отсылает нас к производным. В частности, используя лекционные записи, вы легко можете получить две несложных формулы матричного дифференцирования.\n",
    "\n",
    "1. Если функция $f(\\overline{x}) = \\overline{a}^{T} \\cdot\\overline{x}$, то $f'(\\overline{x}) = \\overline{a} $. Где $\\overline{x},\\overline{a} $ - вектор столбцы.\n",
    "\n",
    "2.  Если функция $f(\\overline{x}) = \\overline{x}^{T} A\\overline{x}$, то $f'(\\overline{x}) = (A+A^{T})\\overline{x} $. Где $\\overline{x}$ - вектор столбец, $A$ - квадратная матрица соответсвующего размера.\n",
    "\n",
    "\n",
    "В будущем, возможно, мы дополним список. А пока можете поупражняться [самостоятельно](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf). \n",
    "\n",
    "\n",
    "Теперь проверим справедливость формулы 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034be24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(10,size =(10,10))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3240e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x): \n",
    "    return np.dot(np.dot(x.T,A),x) #задаём вектор-функцию согласно формуле 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.arange(0,10) # конкретные значения x0\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f341d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = nd.Gradient(func)(x0) #находим производную в точке x0(градиент) встроенными численными методами\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(A + A.T,x0) # а теперь по нашей формуле. Всё сходится."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a43162",
   "metadata": {},
   "source": [
    "<font color = 'red' size = 5>Задание 1 </font>\n",
    "\n",
    "1. Изучите подробно формулы полученные выше. Попрактикуйтесь самостоятельно в их выводе.\n",
    "2. Получите ещё 2 любых формулы для подобного матричного или векторного дифференцирования. Вывод формул кратко оформите в документе.\n",
    "3. Проверьте справедливость полученных вами формул с помощью встроенных функций пакета numdifftools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5666154-ed48-4de6-85ae-70fcf7794e89",
   "metadata": {},
   "source": [
    "## <font color = 'green'> 2. Линейная регрессия по произвольному базису. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96f828-03b7-4709-8f48-5712d9d1c4b7",
   "metadata": {},
   "source": [
    "Пусть задано множество пар признаков(фич) и меток $A = \\{(\\overline{X}_{1},y_{1}),(\\overline{X}_{2},y_{2}),...,(\\overline{X}_{n},y_{n})\\}$. Важно понимать, что теперь каждый элемент множества признаков это вектор состоящий из некоторого числа $k$ признаков  $\\overline{X}_{i} = (x_{1},x_{2},...x_{k})$.\n",
    "\n",
    "Стоит задача получить функцию, позволяющую предсказывать непрерывную метку по $y$ набору признаков $X$. Такую задачу называют множественной регрессией. \n",
    "\n",
    "Для решения данной задачи можно использовать формулу множественной регрессии:$$f(\\overline{x})=\\sum\\limits_{j=1}^m w_{j} \\phi_{j}(\\overline{x}) $$.\n",
    "\n",
    "В приведённой выше формуле, стоит взвешенная сумма некоторых произвольных функций от вектора фич. Это и есть **формула множественной линейной регрессии по произвольному базису**. \n",
    "\n",
    "Частным случаем такой регрессии можно считать тривиальную линейную регрессию, когда $\\phi_{i}(\\overline{X}) = x_{i}$. Тогда добавив фиктивную функцию $\\phi_{0}(\\overline{x}) = 1$ получим формулу **классической линейной регрессии**.\n",
    "\n",
    "$$f(\\overline{x})=w_{0} + \\sum\\limits_{j=1}^k w_{j} x_{j} $$\n",
    "\n",
    "Тут $m = k + 1 $\n",
    "\n",
    "Обратите внимание, что формула парной регресии $f(x) = a x +b $ - тоже частный случай вышеприведённой. \n",
    "\n",
    "Также стоит заметить, *что количество базисных функций может быть как меньшим, так и большим, чем количество изначальных фич*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3bc89-063a-4b74-99bc-652139ac71aa",
   "metadata": {},
   "source": [
    "Теперь приступим к **обучению полученной модели** $f(\\overline{x})=\\sum\\limits_{j=1}^m w_{j} \\phi_{j}(\\overline{x}) $. Оно сводится к нахождению весов $w_{1},...,w_{m}$ на основе исходных обучающих данных.\n",
    "\n",
    "Аналогично парной регрессии составим функцию потерь и минимизируем её: $$ \\mathcal{L}(\\overline{w}) =\\sum\\limits_{i=1}^n (y_{i} - \\sum\\limits_{j=1}^m w_{j} \\phi_{j}(\\overline{X_{i}}))^2 \\rightarrow min$$.\n",
    "\n",
    "Конечно проще записать эту формулу в матричном виде (получите её самостоятельно): $$ \\mathcal{L}(\\overline{w}) = (\\overline{y} - Q\\cdot\\overline{w})^{T}(\\overline{y} - Q\\cdot\\overline{w}), $$\n",
    "где $Q$ - *информационная матрица*, которая определяется следующим образом: $$Q=\\begin{bmatrix}\n",
    "    \\phi_{1}(\\overline{x_{1}})       &  \\phi_{2}(\\overline{x_{1}}) & \\phi_{3}(\\overline{x_{1}}) & \\dots & \\phi_{m}(\\overline{x_{1}}) \\\\\n",
    "    \\phi_{1}(\\overline{x_{2}})       &  \\phi_{2}(\\overline{x_{2}}) & \\phi_{3}(\\overline{x_{2}}) & \\dots & \\phi_{m}(\\overline{x_{2}})   \\\\\n",
    "                            ...\\\\\n",
    "   \\phi_{1}(\\overline{x_{n}})       &  \\phi_{2}(\\overline{x_{n}}) & \\phi_{3}(\\overline{x_{n}}) & \\dots & \\phi_{m}(\\overline{x_{n}}) \n",
    "\\end{bmatrix}, $$\n",
    "\n",
    "$\\overline{w}$ - вектор столбец весов: $$ \\overline{w} = \\begin{bmatrix} \n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "...\\\\\n",
    "w_{m}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$\\overline{y}$ - вектор столбец известных целевых меток: $$ \\overline{y} = \\begin{bmatrix} \n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "...\\\\\n",
    "y_{n}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "Далее решаем задачу оптимизации. Отыщем производную полученной скалярной вектор-функции и приравняем к 0, опираясь на знания из п. 1.$$\\frac{\\partial \\mathcal{L}}{\\partial w} = \\nabla \\mathcal{L} =\\nabla \\mathcal{L}((\\overline{y} - Q\\cdot\\overline{w})(\\overline{y} - Q\\cdot\\overline{w})^{T}) = \\nabla \\mathcal{L} (\\overline{y}^{T} \\overline{y}-2\\overline{y}^{T} Q\\overline{w}+\\overline{w}^{T} Q^{T} Q\\overline{w} ) = 0$$\n",
    "\n",
    "Далее применяем формулы из п. 1. $$\\nabla \\mathcal{L} (\\overline{y}^{T} \\overline{y}-2\\overline{y}^{T} Q\\overline{w}+\\overline{w}^{T} Q^{T} Q\\overline{w} ) = (-2 Q^{T} \\overline{y}+ 2 Q^{T} Q \\overline{w}) = 0.$$\n",
    "\n",
    "Откуда немедленно получаем итоговую формулу для вектора весов: $$\\overline{w} = (Q^{T} Q)^{-1} Q^{T} \\overline{y}$$.\n",
    "\n",
    "\n",
    "Полученная формула имеет некоторые ограничения (подумайте какие). Также следует продумывать выбор базисных функций. Для получения *информационной матрицы* исходные данные требуют некоторого преобразования в зависимости от выбора базисных функций."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70801eb",
   "metadata": {},
   "source": [
    "<font color = 'orange' size = 3>Пример 1 </font>\n",
    "\n",
    "Для начала используем встроенные возможности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0105482",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1) # задаём искусственные данные\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c ='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0aed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression #пробуем обучить стандартную модель регрессии из ЛР1 \n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e52f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x[:, np.newaxis]\n",
    "(X.shape,x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167e876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testx = np.linspace(np.min(x),np.max(x),50)[:, np.newaxis]# по рисунку видим, что получили хрень\n",
    "plt.scatter(x, y, c ='red')\n",
    "plt.plot(testx,model.predict(testx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41b7ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.score(X,y)# позорище, а не модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.coef_,model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f87c50",
   "metadata": {},
   "source": [
    "А теперь добавим другой базис. Видно, что функция похожа на степенную. Используем полиномиальный базис.\n",
    "Класс [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) позволяет преобразовать ваши данные  и перейти от стандартного базиса ${x_1,x_2,...x_n}$ полиномиальному базису, который состоит из всех возможных функций $x_1^{k1}x_2^{k2}...x_n^{kn} : k1+k2+...kn<=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054248ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg=PolynomialFeatures(degree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950805ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = poly_reg.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a68b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new #вместо одного x теперь целый вектор фич степени от 0 до 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "testx = np.linspace(np.min(x),np.max(x),50)[:, np.newaxis]\n",
    "plt.scatter(x, y, c ='red')\n",
    "plt.plot(testx,model.predict(poly_reg.fit_transform(testx)))\n",
    "# теперь для предсказания модели нужно передавать преобразованные в тот же базис данные\n",
    "# результат лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_new,y)# уже лучше!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c2ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(model.coef_,model.intercept_) # а вот и полученные веса для базисных функций. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c153a",
   "metadata": {},
   "source": [
    "**Упражнение** Подумайте, за что отвечает параметр fit_intercept и почему первый коэффициент равен 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b78384",
   "metadata": {},
   "source": [
    "<font color = 'orange' size = 3>Пример 2 </font>\n",
    "\n",
    "Теперь возьмём многомерные фичи. Используем рыбный датасет из файла Fish.csv и попробуем создать модель регрессии для предсказания веса (столбец weight).\n",
    "\n",
    "Попробуем использовать стандартную формулу для классической регрессии $f(\\overline{x})=w_{0} + \\sum\\limits_{j=1}^k w_{i} x_{i} $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_csv(\"Fish.csv\") # загружаем данные\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe.drop([\"Species\",\"Weight\"],axis=1).values # выбираем то, что будет фичами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8830a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d97a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataframe[\"Weight\"].values # вес будем предсказывать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True) # модель регрессии по умолчанию будет использовать стандартный базис\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfff2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb209e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eed529",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.coef_,model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4edd9",
   "metadata": {},
   "source": [
    "<font color = 'red' size = 5>Задание 2 </font>\n",
    "\n",
    "1. Изучите подробно описанные выше примеры.\n",
    "2. Реализуйте функции для работы регрессии по произвольному базису самостоятельно (используйте только базовые функции и numpy), используя формулы, полученные выше. Используйте матричные вычисления!\n",
    "3. Попрактикуйте модели и вашу и встроенную на разных искусственных данных.\n",
    "4. Постройте модели (встроенную и вашу) для прогнозирования веса рыбы из файла Fish.csv. Попробуйте различные базисы, различные комбинации фич и постарайтесь повысить точность прогноза.\n",
    "5. Попытайтесь предсказать [Perfomance index](https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) студентов и [Цену дома (Med house val)](https://www.kaggle.com/datasets/shibumohapatra/house-price/data) используя вашу собственную и встроенные модели с различными базисами. \n",
    "6. Подумайте, как повлияет на модель использование слишком большого количества базисных фич.\n",
    "7. В примере выше мы тестируем полученную модель на тех же данных, что и обучаем. В целом это не очень хорошо. Разделите изначальные данные на обучающую и тестовую выборку и повторите вычисления точности.\n",
    "8. Попробуйте на тестовых или реальных данных повышать сложность модели. Например добавляя фичи всё большей и большей степени. Как это влияет на точность на обучющих и тестовых данных??\n",
    "\n",
    "\n",
    "Указание! Для оценки качества модели, в случае, если рисунок не возможен используйте [коэффициент детерминации](https://wiki.loginom.ru/articles/coefficient-of-determination.html): $$R^{2}=1-\\frac{(\\overline{y} -f(\\overline{x}))(\\overline{y} - f(\\overline{x}))^{T}}{(\\overline{y} - mean(\\overline{y}))(\\overline{y} - mean(\\overline{y}))^{T}},$$\n",
    "\n",
    "где $\\overline{y}$ - столбец обучающих меток, $f(\\overline{x})$ - функция предсказания применённая к вектор столбцу исходных признаков. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18866f",
   "metadata": {},
   "source": [
    "to be continued... 🧡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
